{
  "cacheItemsMap": {
    "posts/quantum-randomness-extractors.mdx": {
      "document": {
        "title": "Quantum-to-Classical Randomness Extractors",
        "date": "2022-12-08T00:00:00.000Z",
        "slug": "quantum-randomness-extractors",
        "summary": "A Deep Dive into Quantum Randomness Extractors and Their Cryptographic Applications",
        "featured": true,
        "body": {
          "raw": "#### Abstract: \n\nRandomness is an essential resource for information theory, cryptography, and\ncomputation. The goal of randomness extraction is to distill (almost) perfect\nrandomness from a weak source of randomness. In this report, we first define\nclassical randomness extractors, or when the source of randomness yields a classical\nstring X. When considering a physical randomness source, X is itself ultimately\nthe result of a measurement on an underlying quantum system, and the question\narises of how much classical randomness can we extract from a quantum system.\nTo understand and analyze this question, we will first provide the relevant quantum\npreliminary background, and then define quantum-to-classical (QC) and quantumto-quantum (QQ) randomness extractors. Finally, we will explore cryptographic\napplications of QC randomness extractors, such as security in the noisy-storage\nmodel, and discuss possible future applications, such as privacy amplification.",
          "code": "var Component=(()=>{var l=Object.create;var r=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var y=(n,s)=>()=>(s||n((s={exports:{}}).exports,s),s.exports),x=(n,s)=>{for(var t in s)r(n,t,{get:s[t],enumerable:!0})},i=(n,s,t,o)=>{if(s&&typeof s==\"object\"||typeof s==\"function\")for(let a of p(s))!f.call(n,a)&&a!==t&&r(n,a,{get:()=>s[a],enumerable:!(o=d(s,a))||o.enumerable});return n};var g=(n,s,t)=>(t=n!=null?l(h(n)):{},i(s||!n||!n.__esModule?r(t,\"default\",{value:n,enumerable:!0}):t,n)),w=n=>i(r({},\"__esModule\",{value:!0}),n);var u=y((b,c)=>{c.exports=_jsx_runtime});var C={};x(C,{default:()=>_,frontmatter:()=>q});var e=g(u()),q={title:\"Quantum-to-Classical Randomness Extractors\",summary:\"A Deep Dive into Quantum Randomness Extractors and Their Cryptographic Applications\",date:\"2022-12-08\",slug:\"quantum-randomness-extractors\",featured:!0};function m(n){let s=Object.assign({h4:\"h4\",p:\"p\"},n.components);return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsx)(s.h4,{children:\"Abstract:\"}),`\n`,(0,e.jsx)(s.p,{children:`Randomness is an essential resource for information theory, cryptography, and\ncomputation. The goal of randomness extraction is to distill (almost) perfect\nrandomness from a weak source of randomness. In this report, we first define\nclassical randomness extractors, or when the source of randomness yields a classical\nstring X. When considering a physical randomness source, X is itself ultimately\nthe result of a measurement on an underlying quantum system, and the question\narises of how much classical randomness can we extract from a quantum system.\nTo understand and analyze this question, we will first provide the relevant quantum\npreliminary background, and then define quantum-to-classical (QC) and quantumto-quantum (QQ) randomness extractors. Finally, we will explore cryptographic\napplications of QC randomness extractors, such as security in the noisy-storage\nmodel, and discuss possible future applications, such as privacy amplification.`})]})}function j(n={}){let{wrapper:s}=n.components||{};return s?(0,e.jsx)(s,Object.assign({},n,{children:(0,e.jsx)(m,n)})):m(n)}var _=j;return w(C);})();\n;return Component;"
        },
        "_id": "posts/quantum-randomness-extractors.mdx",
        "_raw": {
          "sourceFilePath": "posts/quantum-randomness-extractors.mdx",
          "sourceFileName": "quantum-randomness-extractors.mdx",
          "sourceFileDir": "posts",
          "contentType": "mdx",
          "flattenedPath": "posts/quantum-randomness-extractors"
        },
        "type": "Posts"
      },
      "documentHash": "1706208401187",
      "hasWarnings": false,
      "documentTypeName": "Posts"
    },
    "posts/wasserstein-gans.mdx": {
      "document": {
        "title": "Wasserstein Generative Adversarial Networks and Their Applications",
        "date": "2021-11-29T00:00:00.000Z",
        "slug": "wasserstein-gans",
        "summary": "All about Wasserstein GANs.",
        "featured": true,
        "body": {
          "raw": "\n#### Intro:\n\nIn 2014, Ian Goodfellow and his colleagues proposed the machine learning architecture of the\nGenerative Adversarial Network (GAN), a generative model that can produce images, text, or\nmusic. [1] In 2016, Tim Salimans alongside Goodfellow and other OpenAI researchers responded\nto the initial GAN paper with a paper on improved techniques for training GANs that discussed\nmethods to stabilize training and encourage convergence. [2] This research directly led to the\ndevelopment of the Wasserstein GAN (WGAN) and subsequent papers by Martin Arjovsky and\nhis colleagues. [3][4] The Wasserstein GAN aimed to reduce mode collapse and create more\nstable training than the original GAN. Today, there are many interesting applications of\nWasserstein GANs including data augmentation for emotion recognition [5] and singing voice\nsynthesizers [6].",
          "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var A=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),f=(e,n)=>{for(var a in n)r(e,a,{get:n[a],enumerable:!0})},o=(e,n,a,i)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let s of u(n))!p.call(e,s)&&s!==a&&r(e,s,{get:()=>n[s],enumerable:!(i=m(n,s))||i.enumerable});return e};var G=(e,n,a)=>(a=e!=null?h(g(e)):{},o(n||!e||!e.__esModule?r(a,\"default\",{value:e,enumerable:!0}):a,e)),N=e=>o(r({},\"__esModule\",{value:!0}),e);var d=A((w,c)=>{c.exports=_jsx_runtime});var y={};f(y,{default:()=>j,frontmatter:()=>v});var t=G(d()),v={title:\"Wasserstein Generative Adversarial Networks and Their Applications\",date:\"2021-11-29\",summary:\"All about Wasserstein GANs.\",slug:\"wasserstein-gans\",featured:!0};function l(e){let n=Object.assign({h4:\"h4\",p:\"p\"},e.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h4,{children:\"Intro:\"}),`\n`,(0,t.jsx)(n.p,{children:`In 2014, Ian Goodfellow and his colleagues proposed the machine learning architecture of the\nGenerative Adversarial Network (GAN), a generative model that can produce images, text, or\nmusic. [1] In 2016, Tim Salimans alongside Goodfellow and other OpenAI researchers responded\nto the initial GAN paper with a paper on improved techniques for training GANs that discussed\nmethods to stabilize training and encourage convergence. [2] This research directly led to the\ndevelopment of the Wasserstein GAN (WGAN) and subsequent papers by Martin Arjovsky and\nhis colleagues. [3][4] The Wasserstein GAN aimed to reduce mode collapse and create more\nstable training than the original GAN. Today, there are many interesting applications of\nWasserstein GANs including data augmentation for emotion recognition [5] and singing voice\nsynthesizers [6].`})]})}function x(e={}){let{wrapper:n}=e.components||{};return n?(0,t.jsx)(n,Object.assign({},e,{children:(0,t.jsx)(l,e)})):l(e)}var j=x;return N(y);})();\n;return Component;"
        },
        "_id": "posts/wasserstein-gans.mdx",
        "_raw": {
          "sourceFilePath": "posts/wasserstein-gans.mdx",
          "sourceFileName": "wasserstein-gans.mdx",
          "sourceFileDir": "posts",
          "contentType": "mdx",
          "flattenedPath": "posts/wasserstein-gans"
        },
        "type": "Posts"
      },
      "documentHash": "1706208412148",
      "hasWarnings": false,
      "documentTypeName": "Posts"
    }
  }
}
