{
  "title": "Wasserstein Generative Adversarial Networks and Their Applications",
  "date": "2021-11-29T00:00:00.000Z",
  "slug": "wasserstein-gans",
  "summary": "All about Wasserstein GANs.",
  "featured": true,
  "body": {
    "raw": "\n#### Intro:\n\nIn 2014, Ian Goodfellow and his colleagues proposed the machine learning architecture of the\nGenerative Adversarial Network (GAN), a generative model that can produce images, text, or\nmusic. [1] In 2016, Tim Salimans alongside Goodfellow and other OpenAI researchers responded\nto the initial GAN paper with a paper on improved techniques for training GANs that discussed\nmethods to stabilize training and encourage convergence. [2] This research directly led to the\ndevelopment of the Wasserstein GAN (WGAN) and subsequent papers by Martin Arjovsky and\nhis colleagues. [3][4] The Wasserstein GAN aimed to reduce mode collapse and create more\nstable training than the original GAN. Today, there are many interesting applications of\nWasserstein GANs including data augmentation for emotion recognition [5] and singing voice\nsynthesizers [6].",
    "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var A=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),f=(e,n)=>{for(var a in n)r(e,a,{get:n[a],enumerable:!0})},o=(e,n,a,i)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let s of u(n))!p.call(e,s)&&s!==a&&r(e,s,{get:()=>n[s],enumerable:!(i=m(n,s))||i.enumerable});return e};var G=(e,n,a)=>(a=e!=null?h(g(e)):{},o(n||!e||!e.__esModule?r(a,\"default\",{value:e,enumerable:!0}):a,e)),N=e=>o(r({},\"__esModule\",{value:!0}),e);var d=A((w,c)=>{c.exports=_jsx_runtime});var y={};f(y,{default:()=>j,frontmatter:()=>v});var t=G(d()),v={title:\"Wasserstein Generative Adversarial Networks and Their Applications\",date:\"2021-11-29\",summary:\"All about Wasserstein GANs.\",slug:\"wasserstein-gans\",featured:!0};function l(e){let n=Object.assign({h4:\"h4\",p:\"p\"},e.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h4,{children:\"Intro:\"}),`\n`,(0,t.jsx)(n.p,{children:`In 2014, Ian Goodfellow and his colleagues proposed the machine learning architecture of the\nGenerative Adversarial Network (GAN), a generative model that can produce images, text, or\nmusic. [1] In 2016, Tim Salimans alongside Goodfellow and other OpenAI researchers responded\nto the initial GAN paper with a paper on improved techniques for training GANs that discussed\nmethods to stabilize training and encourage convergence. [2] This research directly led to the\ndevelopment of the Wasserstein GAN (WGAN) and subsequent papers by Martin Arjovsky and\nhis colleagues. [3][4] The Wasserstein GAN aimed to reduce mode collapse and create more\nstable training than the original GAN. Today, there are many interesting applications of\nWasserstein GANs including data augmentation for emotion recognition [5] and singing voice\nsynthesizers [6].`})]})}function x(e={}){let{wrapper:n}=e.components||{};return n?(0,t.jsx)(n,Object.assign({},e,{children:(0,t.jsx)(l,e)})):l(e)}var j=x;return N(y);})();\n;return Component;"
  },
  "_id": "posts/wasserstein-gans.mdx",
  "_raw": {
    "sourceFilePath": "posts/wasserstein-gans.mdx",
    "sourceFileName": "wasserstein-gans.mdx",
    "sourceFileDir": "posts",
    "contentType": "mdx",
    "flattenedPath": "posts/wasserstein-gans"
  },
  "type": "Posts"
}